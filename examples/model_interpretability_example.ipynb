{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUSED Framework: Model Interpretability Example\n",
    "\n",
    "This notebook demonstrates how to use the interpretability tools in the FUSED framework to understand and visualize model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Import FUSED utilities\n",
    "from fused.utils.interpretability import FeatureImportance, AttentionVisualization\n",
    "from fused.models import SequentialEncoder, TransformerEncoder, TemporalFusionModel\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable interactive plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Simple Model\n",
    "\n",
    "Let's create a simple model that we can interpret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableModel(nn.Module):\n",
    "    \"\"\"A simple model with attention for interpretability.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=5, seq_length=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_encoder = TransformerEncoder(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=32,\n",
    "            output_dim=32,\n",
    "            num_layers=2,\n",
    "            num_heads=4,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, seq_length, input_dim)\n",
    "        encoded, attention = self.seq_encoder(x, return_attention=True)\n",
    "        logits = self.classifier(encoded)\n",
    "        \n",
    "        return {\"logits\": logits, \"attention\": attention}\n",
    "\n",
    "# Create sample data\n",
    "def generate_sample_data(n_samples=100, seq_length=10, input_dim=5):\n",
    "    # Generate random sequences\n",
    "    X = torch.randn(n_samples, seq_length, input_dim)\n",
    "    \n",
    "    # Create a pattern where the first dimension in the first half of the sequence\n",
    "    # and the second dimension in the second half determine the class\n",
    "    X[:, :seq_length//2, 0] += torch.rand(n_samples, seq_length//2) * 2\n",
    "    X[:, seq_length//2:, 1] += torch.rand(n_samples, seq_length - seq_length//2) * 2\n",
    "    \n",
    "    # Generate labels\n",
    "    first_feature_sum = X[:, :seq_length//2, 0].sum(dim=1)\n",
    "    second_feature_sum = X[:, seq_length//2:, 1].sum(dim=1)\n",
    "    y = ((first_feature_sum + second_feature_sum) > 0).long()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_sample_data(n_samples=100)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Initialize model\n",
    "model = InterpretableModel(input_dim=X.shape[2], seq_length=X.shape[1])\n",
    "\n",
    "# Pretend we've trained the model (for demo purposes)\n",
    "def simulate_trained_model(model, X, y):\n",
    "    # Create a simple pattern in the model weights to simulate training\n",
    "    with torch.no_grad():\n",
    "        # Set weights to make the model pay attention to the first feature in the first half\n",
    "        # and the second feature in the second half\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"self_attn\" in name and \"weight\" in name:\n",
    "                param.data = torch.randn_like(param.data) * 0.01\n",
    "                \n",
    "        # Make the classifier weights emphasize the correct features\n",
    "        model.classifier.weight.data[0, :] = torch.randn(32) * 0.01\n",
    "        model.classifier.weight.data[1, :] = torch.randn(32) * 0.01\n",
    "        model.classifier.weight.data[0, 0] = 0.5  # Class 0 correlation with feature 0\n",
    "        model.classifier.weight.data[1, 1] = 0.5  # Class 1 correlation with feature 1\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Simulate a trained model\n",
    "model = simulate_trained_model(model, X, y)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X)\n",
    "    preds = outputs[\"logits\"].argmax(dim=1)\n",
    "    accuracy = (preds == y).float().mean().item()\n",
    "    \n",
    "print(f\"Model accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Now, let's analyze the feature importance using permutation importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature importance analyzer\n",
    "analyzer = FeatureImportance(model)\n",
    "\n",
    "# Define a metric function for classification\n",
    "def accuracy_metric(y_true, y_pred):\n",
    "    return (y_pred.argmax(dim=1) == y_true).float().mean().item()\n",
    "\n",
    "# Calculate permutation importance\n",
    "feature_names = [f\"Feature_{i}\" for i in range(X.shape[2])]\n",
    "perm_importance = analyzer.permutation_importance(\n",
    "    X, y, accuracy_metric, n_repeats=10,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Plot feature importance\n",
    "fig = analyzer.plot_feature_importance(\n",
    "    perm_importance, \n",
    "    title=\"Feature Importance (Permutation)\",\n",
    "    sort=True\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients Analysis\n",
    "\n",
    "Let's also analyze feature importance using integrated gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate integrated gradients for a single sample\n",
    "sample_idx = 0\n",
    "sample_X = X[sample_idx:sample_idx+1]\n",
    "sample_y = y[sample_idx:sample_idx+1]\n",
    "\n",
    "# Get integrated gradients\n",
    "integrated_grads = analyzer.integrated_gradients(\n",
    "    sample_X, n_steps=50,\n",
    "    target_class=sample_y.item(),\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Plot integrated gradients\n",
    "# Reshape attributions to match sequence structure\n",
    "seq_length = X.shape[1]\n",
    "n_features = X.shape[2]\n",
    "attributions = integrated_grads[\"importances\"].reshape(seq_length, n_features)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(attributions, cmap=\"coolwarm\", center=0, \n",
    "            xticklabels=feature_names,\n",
    "            yticklabels=[f\"Step {i}\" for i in range(seq_length)])\n",
    "plt.title(f\"Integrated Gradients for Sample {sample_idx} (Class {sample_y.item()})\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Sequence Steps\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization\n",
    "\n",
    "Now, let's visualize the attention patterns in our transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention visualizer\n",
    "attention_viz = AttentionVisualization(model)\n",
    "\n",
    "# Register hooks to capture attention weights\n",
    "attention_viz.register_hooks()\n",
    "\n",
    "# Get attention maps for a batch of samples\n",
    "sample_batch = X[:5]  # Use first 5 samples\n",
    "with torch.no_grad():\n",
    "    _ = model(sample_batch)  # Forward pass to trigger hooks\n",
    "    \n",
    "# Get attention maps\n",
    "attention_maps = attention_viz.get_attention_maps(sample_batch)\n",
    "\n",
    "# Plot attention heatmap for the first sample and first head\n",
    "sample_idx = 0\n",
    "for layer_name in attention_maps.keys():\n",
    "    for head_idx in range(4):  # 4 attention heads\n",
    "        fig = attention_viz.plot_attention_heatmap(\n",
    "            attention_maps, \n",
    "            layer_name=layer_name, \n",
    "            head_idx=head_idx, \n",
    "            sample_idx=sample_idx,\n",
    "            title=f\"{layer_name} - Head {head_idx}\"\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "# Remove hooks\n",
    "attention_viz.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Feature Attribution\n",
    "\n",
    "Let's analyze which time steps are most important for the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate attribution for each time step\n",
    "def analyze_temporal_importance(model, X, y):\n",
    "    model.eval()\n",
    "    importance_scores = []\n",
    "    \n",
    "    # Original accuracy\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        preds = outputs[\"logits\"].argmax(dim=1)\n",
    "        original_acc = (preds == y).float().mean().item()\n",
    "    \n",
    "    # Mask each time step and check impact\n",
    "    seq_length = X.shape[1]\n",
    "    for t in range(seq_length):\n",
    "        # Create a copy with the time step masked\n",
    "        X_masked = X.clone()\n",
    "        X_masked[:, t, :] = 0  # Mask with zeros\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_masked)\n",
    "            preds = outputs[\"logits\"].argmax(dim=1)\n",
    "            masked_acc = (preds == y).float().mean().item()\n",
    "        \n",
    "        # Calculate importance as decrease in accuracy\n",
    "        importance = original_acc - masked_acc\n",
    "        importance_scores.append(importance)\n",
    "    \n",
    "    return importance_scores\n",
    "\n",
    "# Analyze temporal importance\n",
    "temporal_importance = analyze_temporal_importance(model, X, y)\n",
    "\n",
    "# Plot importance of each time step\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(temporal_importance)), temporal_importance)\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Importance (Decrease in Accuracy)\")\n",
    "plt.title(\"Temporal Feature Importance\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Model Predictions for a Single Sample\n",
    "\n",
    "Finally, let's combine these approaches to provide a comprehensive interpretation of predictions for a single sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_sample(model, X_sample, y_sample, feature_names):\n",
    "    \"\"\"Combine multiple interpretation methods for a single sample.\"\"\"\n",
    "    model.eval()\n",
    "    analyzer = FeatureImportance(model)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_sample)\n",
    "        logits = outputs[\"logits\"]\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred_class = logits.argmax(dim=1).item()\n",
    "    \n",
    "    print(f\"Sample true class: {y_sample.item()}\")\n",
    "    print(f\"Predicted class: {pred_class}\")\n",
    "    print(f\"Prediction probabilities: {probs[0].numpy()}\")\n",
    "    \n",
    "    # Get integrated gradients\n",
    "    ig_results = analyzer.integrated_gradients(\n",
    "        X_sample, n_steps=50,\n",
    "        target_class=pred_class,\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "    \n",
    "    # Reshape attributions\n",
    "    seq_length = X_sample.shape[1]\n",
    "    n_features = X_sample.shape[2]\n",
    "    attributions = ig_results[\"importances\"].reshape(seq_length, n_features)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot the input features\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.heatmap(X_sample[0].numpy(), cmap=\"viridis\",\n",
    "                xticklabels=feature_names, \n",
    "                yticklabels=[f\"Step {i}\" for i in range(seq_length)])\n",
    "    plt.title(\"Input Features\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Time Steps\")\n",
    "    \n",
    "    # Plot attributions\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.heatmap(attributions, cmap=\"coolwarm\", center=0,\n",
    "                xticklabels=feature_names, \n",
    "                yticklabels=[f\"Step {i}\" for i in range(seq_length)])\n",
    "    plt.title(f\"Feature Attributions for Class {pred_class}\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Time Steps\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get attention maps\n",
    "    attention_viz = AttentionVisualization(model)\n",
    "    attention_viz.register_hooks()\n",
    "    \n",
    "    # Forward pass to get attention\n",
    "    with torch.no_grad():\n",
    "        _ = model(X_sample)\n",
    "    \n",
    "    # Get and plot attention maps\n",
    "    attention_maps = attention_viz.get_attention_maps(X_sample)\n",
    "    for layer_name in attention_maps.keys():\n",
    "        if \"self_attn\" in layer_name:\n",
    "            fig = attention_viz.plot_attention_heatmap(\n",
    "                attention_maps, \n",
    "                layer_name=layer_name, \n",
    "                head_idx=0,  # First head\n",
    "                sample_idx=0,\n",
    "                title=f\"{layer_name} Attention\"\n",
    "            )\n",
    "            plt.show()\n",
    "    \n",
    "    # Remove hooks\n",
    "    attention_viz.remove_hooks()\n",
    "\n",
    "# Select a sample to interpret\n",
    "sample_idx = 10\n",
    "interpret_sample(\n",
    "    model, \n",
    "    X[sample_idx:sample_idx+1], \n",
    "    y[sample_idx:sample_idx+1],\n",
    "    feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the interpretability tools in the FUSED framework to understand and visualize model predictions. These tools help to:\n",
    "\n",
    "1. Identify which features are most important for predictions\n",
    "2. Understand how the model attends to different parts of the input sequence\n",
    "3. Visualize feature attributions for individual predictions\n",
    "4. Analyze the temporal importance of different time steps\n",
    "\n",
    "By using these interpretability techniques, you can gain insights into how your models work, debug issues, and increase trust in model predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
