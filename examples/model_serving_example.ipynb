{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUSED Framework: Model Serving Example\n",
    "\n",
    "This notebook demonstrates how to export and serve models created with the FUSED framework for deployment in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Import FUSED utilities\n",
    "from fused.utils.serving import ModelExporter, ModelServer, load_model\n",
    "from fused.models import SequentialEncoder\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Simple Model\n",
    "\n",
    "Let's create a simple FUSED model for time series classification that we'll export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesClassifier(nn.Module):\n",
    "    \"\"\"A simple time series classifier model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=5, hidden_dim=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"num_classes\": num_classes\n",
    "        }\n",
    "        \n",
    "        self.encoder = SequentialEncoder(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=hidden_dim,\n",
    "            bidirectional=True,\n",
    "            encoder_type=\"lstm\"\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Handle both tensor and dictionary inputs\n",
    "        if isinstance(x, dict):\n",
    "            if \"features\" in x:\n",
    "                x = x[\"features\"]\n",
    "            else:\n",
    "                x = next(iter(x.values()))\n",
    "                \n",
    "        # Encode sequence\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(encoded)\n",
    "        \n",
    "        # Return dictionary output\n",
    "        return {\"logits\": logits, \"embeddings\": encoded}\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Convenience method for getting predictions.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self(x)\n",
    "            probs = torch.softmax(outputs[\"logits\"], dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "        return {\"class\": preds, \"probabilities\": probs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "Let's create some synthetic time series data for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=100, seq_length=20, input_dim=5):\n",
    "    \"\"\"Generate synthetic time series data.\"\"\"\n",
    "    # Generate random sequences\n",
    "    X = torch.randn(n_samples, seq_length, input_dim)\n",
    "    \n",
    "    # Add a pattern where the first feature determines the class\n",
    "    pattern = torch.sin(torch.linspace(0, 4*np.pi, seq_length)).unsqueeze(0).unsqueeze(-1)\n",
    "    \n",
    "    # Class 0: sinusoidal pattern\n",
    "    # Class 1: negative sinusoidal pattern\n",
    "    labels = torch.randint(0, 2, (n_samples,))\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 0:\n",
    "            X[i, :, 0:1] += pattern\n",
    "        else:\n",
    "            X[i, :, 0:1] -= pattern\n",
    "    \n",
    "    return X, labels\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_synthetic_data(n_samples=100, seq_length=20, input_dim=5)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = 80\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Let's quickly train our model on the synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = TimeSeriesClassifier(input_dim=X.shape[2], hidden_dim=32, num_classes=2)\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs[\"logits\"], labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs[\"logits\"], 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Print epoch metrics\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train(model, train_loader, criterion, optimizer, num_epochs=5)\n",
    "\n",
    "# Evaluate on test set\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs[\"logits\"], 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Model\n",
    "\n",
    "Now, let's use the `ModelExporter` to save our model in different formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create export directory\n",
    "export_dir = \"./exported_models\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Create model exporter\n",
    "exporter = ModelExporter(model, save_dir=export_dir)\n",
    "\n",
    "# Export in PyTorch format\n",
    "pt_path = exporter.export_pytorch(filename=\"time_series_classifier.pt\")\n",
    "print(f\"PyTorch model exported to: {pt_path}\")\n",
    "\n",
    "# Export model configuration\n",
    "config_path = exporter.export_config(filename=\"model_config.json\")\n",
    "print(f\"Model configuration exported to: {config_path}\")\n",
    "\n",
    "# Example inputs for TorchScript export\n",
    "example_batch = next(iter(test_loader))[0]\n",
    "\n",
    "# Export in TorchScript format\n",
    "try:\n",
    "    script_path = exporter.export_torchscript(\n",
    "        filename=\"time_series_classifier_script.pt\",\n",
    "        example_inputs=example_batch,\n",
    "        method=\"trace\"\n",
    "    )\n",
    "    print(f\"TorchScript model exported to: {script_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"TorchScript export failed: {e}\")\n",
    "\n",
    "# Export in ONNX format (if available)\n",
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime\n",
    "    \n",
    "    onnx_path = exporter.export_onnx(\n",
    "        filename=\"time_series_classifier.onnx\",\n",
    "        example_inputs=example_batch,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"logits\", \"embeddings\"]\n",
    "    )\n",
    "    print(f\"ONNX model exported to: {onnx_path}\")\n",
    "except (ImportError, Exception) as e:\n",
    "    print(f\"ONNX export skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test the Exported Model\n",
    "\n",
    "Let's load the exported model and verify that it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = load_model(pt_path)\n",
    "\n",
    "# Verify the loaded model\n",
    "print(f\"Loaded model: {type(loaded_model)}\")\n",
    "\n",
    "# Compare predictions\n",
    "example_input = X_test[:5]  # Take a few test samples\n",
    "\n",
    "# Original model prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    original_output = model(example_input)\n",
    "    original_preds = torch.argmax(original_output[\"logits\"], dim=1)\n",
    "\n",
    "# Loaded model prediction\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    loaded_output = loaded_model(example_input)\n",
    "    loaded_preds = torch.argmax(loaded_output[\"logits\"], dim=1)\n",
    "\n",
    "# Compare predictions\n",
    "print(f\"Original model predictions: {original_preds}\")\n",
    "print(f\"Loaded model predictions: {loaded_preds}\")\n",
    "print(f\"Predictions match: {torch.all(original_preds == loaded_preds).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model Server\n",
    "\n",
    "Now, let's create a server to serve our model via HTTP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model server\n",
    "server = ModelServer(model_path=pt_path, device=\"cpu\")\n",
    "\n",
    "# Define preprocessing and postprocessing functions\n",
    "def preprocess(data):\n",
    "    \"\"\"Convert input data to tensor format.\"\"\"\n",
    "    if isinstance(data, dict) and \"data\" in data:\n",
    "        # Convert nested arrays to tensors\n",
    "        if isinstance(data[\"data\"], list):\n",
    "            return torch.tensor(data[\"data\"], dtype=torch.float32)\n",
    "        return data[\"data\"]\n",
    "    return data\n",
    "\n",
    "def postprocess(output):\n",
    "    \"\"\"Convert model output to JSON-serializable format.\"\"\"\n",
    "    result = {}\n",
    "    for k, v in output.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            result[k] = v.cpu().numpy().tolist()\n",
    "        else:\n",
    "            result[k] = v\n",
    "    return result\n",
    "\n",
    "# Configure server\n",
    "server.set_preprocessing(preprocess)\n",
    "server.set_postprocessing(postprocess)\n",
    "\n",
    "# Start server in a separate thread\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def run_server():\n",
    "    server.start_http_server(host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Start server thread\n",
    "server_thread = threading.Thread(target=run_server)\n",
    "server_thread.daemon = True  # This ensures the thread will be terminated when the notebook is closed\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to start\n",
    "time.sleep(2)\n",
    "print(\"Server started at http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Server API\n",
    "\n",
    "Let's test our server by sending requests to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare example data\n",
    "example_data = X_test[0:1].numpy().tolist()  # Convert to list for JSON serialization\n",
    "\n",
    "# Send prediction request\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/predict\",\n",
    "    json={\"data\": example_data}\n",
    ")\n",
    "\n",
    "# Check response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Server response:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "    # Verify prediction\n",
    "    with torch.no_grad():\n",
    "        direct_pred = model(torch.tensor(example_data, dtype=torch.float32))\n",
    "        direct_class = torch.argmax(direct_pred[\"logits\"], dim=1).item()\n",
    "    \n",
    "    server_logits = result[\"logits\"][0]\n",
    "    server_class = server_logits.index(max(server_logits))\n",
    "    \n",
    "    print(f\"\\nDirect model prediction: Class {direct_class}\")\n",
    "    print(f\"Server prediction: Class {server_class}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction API\n",
    "\n",
    "Let's also test the batch prediction API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch data\n",
    "batch_data = X_test[0:5].numpy().tolist()  # Convert to list for JSON serialization\n",
    "\n",
    "# Send batch prediction request\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/predict_batch\",\n",
    "    json={\"data\": batch_data}\n",
    ")\n",
    "\n",
    "# Check response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Server batch response (truncated):\")\n",
    "    print(json.dumps({\n",
    "        \"logits\": result[\"logits\"][0:2],  # Show just first two predictions\n",
    "        \"embeddings\": \"...\"\n",
    "    }, indent=2))\n",
    "    \n",
    "    # Verify batch prediction\n",
    "    with torch.no_grad():\n",
    "        direct_pred = model(torch.tensor(batch_data, dtype=torch.float32))\n",
    "        direct_classes = torch.argmax(direct_pred[\"logits\"], dim=1).tolist()\n",
    "    \n",
    "    server_classes = [logits.index(max(logits)) for logits in result[\"logits\"]]\n",
    "    \n",
    "    print(f\"\\nDirect model batch predictions: {direct_classes}\")\n",
    "    print(f\"Server batch predictions: {server_classes}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding API\n",
    "\n",
    "Let's test the embedding extraction API as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send embedding extraction request\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/embed\",\n",
    "    json={\"data\": example_data}\n",
    ")\n",
    "\n",
    "# Check response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Embedding response:\")\n",
    "    print(f\"Embedding shape: {len(result['embeddings'][0])} dimensions\")\n",
    "    print(f\"First few dimensions: {result['embeddings'][0][:5]}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metadata API\n",
    "\n",
    "Finally, let's check the model metadata API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model metadata\n",
    "response = requests.get(\"http://localhost:8000/metadata\")\n",
    "\n",
    "# Check response\n",
    "if response.status_code == 200:\n",
    "    metadata = response.json()\n",
    "    print(\"Model metadata:\")\n",
    "    print(json.dumps(metadata, indent=2))\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown Server\n",
    "\n",
    "Now let's shut down the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the server\n",
    "requests.post(\"http://localhost:8000/shutdown\")\n",
    "print(\"Server shutdown requested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Options\n",
    "\n",
    "There are several ways to deploy FUSED models in production environments:\n",
    "\n",
    "1. **HTTP Server**: As demonstrated above, using Flask-based HTTP server for REST API access\n",
    "2. **Docker Container**: Package the model and server in a Docker container for easy deployment\n",
    "3. **Cloud Platforms**: Deploy to AWS SageMaker, Azure ML, or Google AI Platform\n",
    "4. **TorchServe**: Use TorchServe for more advanced serving capabilities\n",
    "5. **ONNX Runtime**: Deploy ONNX models with ONNX Runtime for cross-platform inference\n",
    "\n",
    "Below is an example of how to create a Dockerfile for deploying our model server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.8-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Copy model and server code\n",
    "COPY exported_models /app/exported_models\n",
    "COPY server.py .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run server\n",
    "CMD [\"python\", \"server.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile server.py\n",
    "import torch\n",
    "from fused.utils.serving import ModelServer, load_model\n",
    "import json\n",
    "import os\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\"Convert input data to tensor format.\"\"\"\n",
    "    if isinstance(data, dict) and \"data\" in data:\n",
    "        # Convert nested arrays to tensors\n",
    "        if isinstance(data[\"data\"], list):\n",
    "            return torch.tensor(data[\"data\"], dtype=torch.float32)\n",
    "        return data[\"data\"]\n",
    "    return data\n",
    "\n",
    "def postprocess(output):\n",
    "    \"\"\"Convert model output to JSON-serializable format.\"\"\"\n",
    "    result = {}\n",
    "    for k, v in output.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            result[k] = v.cpu().numpy().tolist()\n",
    "        else:\n",
    "            result[k] = v\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the model\n",
    "    model_path = os.path.join(\"exported_models\", \"time_series_classifier.pt\")\n",
    "    \n",
    "    # Create server\n",
    "    server = ModelServer(model_path=model_path, device=\"cpu\")\n",
    "    \n",
    "    # Configure server\n",
    "    server.set_preprocessing(preprocess)\n",
    "    server.set_postprocessing(postprocess)\n",
    "    \n",
    "    # Start server\n",
    "    server.start_http_server(host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "torch>=1.10.0\n",
    "fused\n",
    "flask>=2.0.0\n",
    "numpy>=1.20.0\n",
    "requests>=2.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the model serving utilities in the FUSED framework to export and deploy models for production use. We've covered:\n",
    "\n",
    "1. Training a simple time series classification model\n",
    "2. Exporting the model in various formats (PyTorch, TorchScript, ONNX)\n",
    "3. Creating a model server with HTTP API endpoints\n",
    "4. Testing the server with prediction, batch prediction, and embedding API calls\n",
    "5. Preparing for containerized deployment with Docker\n",
    "\n",
    "The `ModelExporter` and `ModelServer` classes make it easy to productionize FUSED models, allowing you to move from research to deployment with minimal effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
